{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"../config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 19:29:44,197\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 19:29:50 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "WARNING 11-19 19:29:50 config.py:428] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 11-19 19:29:50 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='Qwen/Qwen1.5-7B-Chat-AWQ', speculative_config=None, tokenizer='Qwen/Qwen1.5-7B-Chat-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen1.5-7B-Chat-AWQ, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 11-19 19:29:51 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-19 19:29:51 selector.py:144] Using XFormers backend.\n",
      "INFO 11-19 19:29:52 model_runner.py:1072] Starting to load model Qwen/Qwen1.5-7B-Chat-AWQ...\n",
      "INFO 11-19 19:29:53 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.18s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.11it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 19:31:30 model_runner.py:1077] Loading model weights took 5.5097 GB\n",
      "INFO 11-19 19:31:35 worker.py:232] Memory profiling results: total_gpu_memory=14.57GiB initial_memory_usage=5.87GiB peak_torch_memory=6.96GiB memory_usage_post_profile=5.88GiB non_torch_memory=0.37GiB kv_cache_size=6.52GiB gpu_memory_utilization=0.95\n",
      "INFO 11-19 19:31:35 gpu_executor.py:113] # GPU blocks: 834, # CPU blocks: 512\n",
      "INFO 11-19 19:31:35 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 1.63x\n",
      "INFO 11-19 19:31:37 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-19 19:31:37 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-19 19:32:08 model_runner.py:1518] Graph capturing finished in 30 secs, took 0.50 GiB\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"HFTOKEN\")\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(**config.get(\"llm_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it, est. speed input: 591.46 toks/s, output: 11.83 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "Resume:\n",
    "\n",
    "Data Scientist - Intern\t07/2023 – 12/2023\n",
    "●\tDeveloped and tuned models to enhance omnichannel marketing communication, building upon previous research findings, spearheading two impactful projects—Omnichannel Marketing Optimization and Marketing Mix Modeling (MMx).\n",
    "●\tTuned and deployed attention-based model for Omnichannel Marketing Optimization, leveraging the transformer architecture for marketing sequence classification, yielding in a predictive tool for business to better plan future marketing activities. \n",
    "●\tAssisted in implementation Mix Market Model ROI forecasting models for marketing data, contributing to informed decision-making and future campaign budget planning. Led initiatives to performance optimize MMx pipeline, integrating GPU acceleration and PySpark implementation, resulting in improved runtime efficiency.\n",
    "\n",
    "Sanofi | Atos\t Paris\n",
    "Corporate Researcher: Marketing Optimization\t01/2023 – 06/2023\n",
    "●\tPerformed a comprehensive research project in partnership with Atos for Sanofi, aimed at enhancing marketing communication strategies directed at healthcare practitioners through the analysis of available data. Conducted an extensive evaluation of prevailing marketing attribution and sequencing techniques.\n",
    "●\tGained understanding of the existing Snowflake-based data architecture and contributed to the development of data pipelines for preparing model-ready data, facilitating future implementation of machine learning solutions.\n",
    "●\tDevised a regression-based statistical model to assess and prioritize the impact of marketing initiatives, laying the groundwork for enhanced business insights. Proposed a range of machine learning and deep learning models for potential integration to optimize marketing strategies.\n",
    "\n",
    "Deloitte US\t Bengaluru\n",
    "Data Analyst – Supply Chain (PLM/MES)\t01/2019 –/07/2022\n",
    "●\tAided in the digital transformation of multiple clients by facilitating data migration and automation for SAP PLM.\n",
    "●\tSuccessfully implemented MES systems in operational factories for a major electronics manufacturer, ensuring uninterrupted production and effectively managing stakeholder expectations.\n",
    "●\tContributed to the implementation of smart factory MES solutions for an aerospace client, leading to optimized shopfloor planning through critical dashboard solutions, resulting in reduced inventory levels and enhanced workforce skill mapping.\n",
    "●\tEngineered and implemented innovative solutions, such as ML-based order failure reduction, RPA integration for ERP, and intelligent ETL techniques, resulting in significant efficiency improvements and acknowledgment from clients.\n",
    "●\tExcelled in Deloitte's supply chain division, achieving consistent performance above the 90th percentile across three years, earning two promotions to consultant.\n",
    "\n",
    "EDUCATION\n",
    "CentraleSupélec & ESSEC Business School\tParis\n",
    "Master of Science in Data Science and Business Analytics\t08/2022 – 12/2023\n",
    "●\tMajor in Data Science with minor in Business Analytics with overall GPA of 15.2/20\n",
    "●\tWinner of cohort Hackathon\n",
    "\n",
    "Manipal Institute of Technology\tManipal\n",
    "Bachelor of Technology in Mechanical and Manufacturing Engineering\t08/2015 – 06/2019\n",
    "●\tMinor in machine design with overall GPA of 7/10\n",
    "\n",
    "PROJECT WORK    \n",
    "CentraleSupélec\tParis\n",
    "Recommender System with LLM\t01/2023 – 06/2023\n",
    "●\tSpearheaded the design and implementation of a predictive model, including Cosine Similarity Collaborative Filtering, Graph Method and Content-Based Filtering using encoder only model, optimizing book recommendations based on user behavior, preferences, and content. \n",
    "●\tConducted in-depth evaluation and performance metrics analysis of four distinct recommendation models, identifying trade-offs between hit rates and precision, with a focus on the content-based model showing the highest hit rate and the joint model demonstrating superior precision.\n",
    "\n",
    "Kaggle\tParis\n",
    "LLM Generated Text Detection\t10/2023 – 01/2024\n",
    "●\tDeveloped pipeline for training a custom tokenizer for text classification with better token understanding by strategic error correction.\n",
    "●\tParticipated in the Kaggle challenge, winning a bronze medal for the same.\n",
    "\n",
    "CERTIFICATIONS & ACHIEVEMENTS\n",
    "●\tKaggle Expert (Competition & Notebook)\n",
    "●\tWinner of DSBA cohort Hackathon \n",
    "●\tSpecialization: Mathematics for Machine Learning (Imperial College – London | Coursera)\n",
    "●\tGRE (Score 323), IELTS (Band 7.5)\n",
    "\n",
    "SKILLS & INTERESTS\n",
    "Interest: Machine Learning, Deep Learning, Optimization, Graph Methods, Data Mining, ETL, Supply Chain Analytics, Data Engineering, ML Ops\n",
    "Skills: Python, SQL, R, PySpark, Pytorch, Keras TF, Statistics, Tableau, PowerBI, Databricks, Snowflake, Alteryx, SAP BODS, SAP PLM, Siemens OpCenter\n",
    "\n",
    "\n",
    "what bachelors degree did the candidate persue\n",
    "'''\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=2048, temperature=0.6, top_p=0.6)\n",
    "response = llm.generate(prompt, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The candidate pursued a Bachelor of Technology in Mechanical and Manufacturing Engineering from Manipal Institute of Technology.\n"
     ]
    }
   ],
   "source": [
    "print(response[0].outputs[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = ['1. dcdasdcscasd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m responses \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m responses]\n",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m responses \u001b[38;5;241m=\u001b[39m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m responses]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'outputs'"
     ]
    }
   ],
   "source": [
    "responses = [x.outputs[0].text for x in responses]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
