# data_processing_pipeline configs
paths:
  resume_data_csv: "..//data//Resume"

embedding_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  path: "../data/embedding_model"

llm_model:
  model: "Qwen/Qwen2.5-7B-Instruct-AWQ"
  quantization: "awq"
  # dtype: "half"
  gpu_memory_utilization: 0.95
  tensor_parallel_size: 1
  trust_remote_code: true
  # enforce_eager: false
  max_model_len: 8192

llm_model_prompt:
  